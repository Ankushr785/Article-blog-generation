{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_word):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_word = [ix_to_word[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_word[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_word.append(ix_to_word[ix[-1]])\n",
    "    return ('').join(y_word)\n",
    "\n",
    "# method for preparing the training data\n",
    "def load_data(data, seq_length):\n",
    "    vocab = []\n",
    "    word = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            word.append(data[i][j])\n",
    "            if data[i][j] not in vocab:\n",
    "                vocab.append(data[i][j])\n",
    "    #chars = list(set(data))\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "\n",
    "    print('Data length: {} words'.format(len(word)))\n",
    "    print('Vocabulary size: {} words'.format(VOCAB_SIZE))\n",
    "\n",
    "    ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "    word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "\n",
    "    X = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "    for i in range(0, int(len(data)/seq_length)):\n",
    "        X_sequence = word[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [word_to_ix[value] for value in X_sequence]\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence = word[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [word_to_ix[value] for value in y_sequence]\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "    return X, y, VOCAB_SIZE, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 146709 words\n",
      "Vocabulary size: 15144 words\n",
      "Knox"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WEIGHTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ed17b1ec536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mWEIGHTS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WEIGHTS' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import collections\n",
    "import nltk\n",
    "#from RNN_utils import *\n",
    "\n",
    "\n",
    "articles1 = pd.read_csv('articles1.csv')\n",
    "articles = articles1.iloc[:100,:]\n",
    "\n",
    "def cleaner(x):\n",
    "    x = nltk.word_tokenize(str(x))\n",
    "    return x\n",
    "\n",
    "articles.content = articles.content.map(lambda x:cleaner(x))\n",
    "articles.title = articles.title.map(lambda x:cleaner(x))\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "unique = []\n",
    "num_recs = 0\n",
    "for line in range(len(articles.content)):\n",
    "    words = articles.content[line]\n",
    "    if len(words)>maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "        if word not in unique:\n",
    "            unique.append(word)\n",
    "    num_recs += 1\n",
    "\n",
    "\n",
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_word = load_data(articles.content, maxlen)\n",
    "\n",
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "model.add(LSTM(1200, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(2):\n",
    "  model.add(LSTM(600, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, 1, VOCAB_SIZE, ix_to_word)\n",
    "\n",
    "if not WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "else:\n",
    "  nb_epoch = 0\n",
    "\n",
    "# Training if there is no trained weights specified\n",
    "if args['mode'] == 'train' or WEIGHTS == '':\n",
    "  while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "      model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "\n",
    "# Else, loading the trained weights and performing generation only\n",
    "elif WEIGHTS == '':\n",
    "  # Loading the trained weights\n",
    "  model.load_weights(WEIGHTS)\n",
    "  generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_word)\n",
    "  print('\\n\\n')\n",
    "else:\n",
    "  print('\\n\\nNothing to do!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5607"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles.content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
