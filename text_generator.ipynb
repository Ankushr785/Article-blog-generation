{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#import fasttext as ft\n",
    "import re\n",
    "import itertools\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1 = pd.read_csv('articles_with_topics.csv')\n",
    "articles = articles1.iloc[:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "def cleaner(x):\n",
    "    x = nltk.word_tokenize(str(x))\n",
    "    #x = [w for w in x if not w in stopset]\n",
    "    x = [z.lower() for z in x]\n",
    "    return x\n",
    "\n",
    "articles.content = articles.content.map(lambda x:cleaner(x))\n",
    "articles.title = articles.title.map(lambda x:cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>[house, republicans, fret, about, winning, the...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[washington, —, congressional, republicans, ha...</td>\n",
       "      <td>['republican', 'administration', 'health', 'su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>[rift, between, officers, and, residents, as, ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[after, the, bullet, shells, get, counted, ,, ...</td>\n",
       "      <td>['police', 'band', 'arrest', 'crime', 'percent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>[tyrus, wong, ,, ‘, bambi, ’, artist, thwarted...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[when, walt, disney, ’, s, “, bambi, ”, opened...</td>\n",
       "      <td>['1942', 'wong', 'artistic', 'miserable', 'chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>[among, deaths, in, 2016, ,, a, heavy, toll, i...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[death, may, be, the, great, equalizer, ,, but...</td>\n",
       "      <td>['george', 'led', 'death', '“the', 'were', 'zs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>[kim, jong-un, says, north, korea, is, prepari...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[seoul, ,, south, korea, —, north, korea, ’, s...</td>\n",
       "      <td>['ballistic', 'test', 'korea', 'missile', '1',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  [house, republicans, fret, about, winning, the...   \n",
       "1           1  17284  [rift, between, officers, and, residents, as, ...   \n",
       "2           2  17285  [tyrus, wong, ,, ‘, bambi, ’, artist, thwarted...   \n",
       "3           3  17286  [among, deaths, in, 2016, ,, a, heavy, toll, i...   \n",
       "4           4  17287  [kim, jong-un, says, north, korea, is, prepari...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \\\n",
       "0  NaN  [washington, —, congressional, republicans, ha...   \n",
       "1  NaN  [after, the, bullet, shells, get, counted, ,, ...   \n",
       "2  NaN  [when, walt, disney, ’, s, “, bambi, ”, opened...   \n",
       "3  NaN  [death, may, be, the, great, equalizer, ,, but...   \n",
       "4  NaN  [seoul, ,, south, korea, —, north, korea, ’, s...   \n",
       "\n",
       "                                              topics  \n",
       "0  ['republican', 'administration', 'health', 'su...  \n",
       "1  ['police', 'band', 'arrest', 'crime', 'percent...  \n",
       "2  ['1942', 'wong', 'artistic', 'miserable', 'chi...  \n",
       "3  ['george', 'led', 'death', '“the', 'were', 'zs...  \n",
       "4  ['ballistic', 'test', 'korea', 'missile', '1',...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 682946 words\n",
      "Vocabulary size: 28918 words\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "word = []\n",
    "for i in range(len(articles.content)):\n",
    "    for j in range(len(articles.content[i])):\n",
    "        word.append(articles.content[i][j])\n",
    "        if articles.content[i][j] not in vocab:\n",
    "            vocab.append(articles.content[i][j])\n",
    "#chars = list(set(data))\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "seq_length = 2\n",
    "print('Data length: {} words'.format(len(word)))\n",
    "print('Vocabulary size: {} words'.format(VOCAB_SIZE))\n",
    "\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "    \n",
    "X = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "y = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "y_bar = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "for i in range(0, int(len(articles.content)/seq_length)):\n",
    "    X_sequence = word[i*seq_length:(i+1)*seq_length]\n",
    "    X_sequence_ix = [word_to_ix[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "        X[i] = input_sequence\n",
    "\n",
    "    y_sequence = word[i*seq_length+1:(i+1)*seq_length+1]\n",
    "    y_sequence_ix = [word_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1\n",
    "        y[i] = target_sequence\n",
    "        \n",
    "    y_bar_sequence = word[i*seq_length-1:(i+1)*seq_length-1]\n",
    "    y_bar_sequence_ix = [word_to_ix[value] for value in y_sequence]\n",
    "    target_sequence_bar = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        target_sequence_bar[j][y_sequence_ix[j]] = 1\n",
    "        y_bar[i] = target_sequence_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = X[:int(.75*len(X)), :, :]\n",
    "ytrain = y[:int(.75*len(X)), :, :]\n",
    "xtest = X[int(.75*len(X)):, :, :]\n",
    "ytest = y[int(.75*len(X)):, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[:int(.75*len(X)), :, :]\n",
    "y_train = y_bar[:int(.75*len(X)), :, :]\n",
    "x_test = X[int(.75*len(X)):, :, :]\n",
    "y_test = y_bar[int(.75*len(X)):, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(5000, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(6):\n",
    "  model.add(LSTM(int(4500/(i+1)), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256104 samples, validate on 85369 samples\n",
      "Epoch 1/10\n",
      "256104/256104 [==============================] - 8243s 32ms/step - loss: 0.0100 - acc: 0.0391 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "256104/256104 [==============================] - 8267s 32ms/step - loss: 0.0100 - acc: 7.4189e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "256104/256104 [==============================] - 8279s 32ms/step - loss: 0.0100 - acc: 7.4189e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "256104/256104 [==============================] - 8214s 32ms/step - loss: 0.0099 - acc: 7.4189e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "256104/256104 [==============================] - 8212s 32ms/step - loss: 0.0092 - acc: 7.4189e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "256104/256104 [==============================] - 8209s 32ms/step - loss: 0.0077 - acc: 5.8570e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "256104/256104 [==============================] - 8210s 32ms/step - loss: 0.0063 - acc: 6.6379e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "256104/256104 [==============================] - 8181s 32ms/step - loss: 0.0055 - acc: 5.2713e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "256104/256104 [==============================] - 8206s 32ms/step - loss: 0.0052 - acc: 6.6379e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "256104/256104 [==============================] - 8207s 32ms/step - loss: 0.0051 - acc: 7.4189e-05 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Training 1 time: 82233.3471930027\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10000\n",
    "training_time1 = time.time()\n",
    "history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, verbose=1, epochs=10, validation_data=(xtest, ytest))\n",
    "end_time1 = time.time()\n",
    "print('Training 1 time:', end_time1-training_time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.hdf5', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judges the the the the the\n"
     ]
    }
   ],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length = 5\n",
    "ix = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word = [ix_to_word[ix[-1]]]\n",
    "x = np.zeros((1, length, VOCAB_SIZE))\n",
    "for i in range(length):\n",
    "    # appending the last predicted character to sequence\n",
    "    x[0, i, :][ix[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "    y_word.append(ix_to_word[ix[-1]])\n",
    "    if y_word[-1] in ['.', '?', '!']:\n",
    "        break\n",
    "print((' ').join(y_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(5000, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(6):\n",
    "  model2.add(LSTM(int(4500/(i+1)), return_sequences=True))\n",
    "model2.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256104 samples, validate on 85369 samples\n",
      "Epoch 1/10\n",
      "120000/256104 [=============>................] - ETA: 1:05:03 - loss: 0.0098 - acc: 0.0833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d332372f06c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE_BAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtraining_time2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_BAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend_time2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time 2:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtraining_time2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_BAR = 10000\n",
    "training_time2 = time.time()\n",
    "history_bar = model2.fit(x_train, y_train, batch_size=BATCH_SIZE_BAR, verbose=1, epochs=10, validation_data=(x_test, y_test))\n",
    "end_time2 = time.time()\n",
    "print('Training time 2:', end_time2-training_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights('model2.hdf5', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length_bar = 5\n",
    "ix_bar = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "for i in range(length_bar):\n",
    "    # appending the last predicted character to sequence\n",
    "    x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "    y_word_bar.append(ix_to_word[ix_bar[-1]])\n",
    "    if y_word_bar[-1] in ['.', '?', '!']:\n",
    "        del y_word_bar[-1]\n",
    "        break\n",
    "    \n",
    "y_word_bar = reversed(y_word_bar)\n",
    "print((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history_bar.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history_bar.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history_bar.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history_bar.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload model\n",
    "model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 1000\n",
    "#history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, verbose=1, epochs=10, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length = 5\n",
    "ix = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word = [ix_to_word[ix[-1]]]\n",
    "x = np.zeros((1, length, VOCAB_SIZE))\n",
    "for i in range(length):\n",
    "    # appending the last predicted character to sequence\n",
    "    x[0, i, :][ix[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "    y_word.append(ix_to_word[ix[-1]])\n",
    "    if y_word[-1] in ['.', '?', '!']:\n",
    "        break\n",
    "print((' ').join(y_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload model\n",
    "model2.load_weights('model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE_BAR = 1000\n",
    "#history = model2.fit(x_train, y_train, batch_size=BATCH_SIZE_BAR, verbose=1, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length_bar = 5\n",
    "ix_bar = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "for i in range(length_bar):\n",
    "    # appending the last predicted character to sequence\n",
    "    x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "    y_word_bar.append(ix_to_word[ix_bar[-1]])\n",
    "    if y_word_bar[-1] in ['.', '?', '!']:\n",
    "        del y_word_bar[-1]\n",
    "        break\n",
    "    \n",
    "y_word_bar = reversed(y_word_bar)\n",
    "print((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(str(\"Enter some keywords or sentences. In case you're entering keywords, don't use comma separation.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keywords.split()\n",
    "for i in keys:\n",
    "    if i not in word_to_ix:\n",
    "        keys.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if keys is None:\n",
    "    print('Error! No keyword recognized!')\n",
    "else:\n",
    "    forward_sentences = []\n",
    "    backward_sentences = []\n",
    "    for m in keys:\n",
    "        length = 5\n",
    "        ix = [word_to_ix[m]]\n",
    "        y_word = [ix_to_word[ix[-1]]]\n",
    "        relevant_words = []\n",
    "        for j in range(len(articles)):\n",
    "            if y_word[0] in articles.topics[j]:\n",
    "                for k in range(len(articles.content[j])):\n",
    "                    relevant_words.append(articles.content[j][k])\n",
    "        x = np.zeros((1, length, VOCAB_SIZE))\n",
    "        for i in range(length):\n",
    "            # appending the last predicted character to sequence\n",
    "            x[0, i, :][ix[-1]] = 1\n",
    "            #print(ix_to_word[ix[-1]], end=\"\")\n",
    "            ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "            for l in range(len(ix)):\n",
    "                if ix_to_word[ix[-1-l]] in relevant_words:\n",
    "                    y_word.append(ix_to_word[ix[-1-l]])\n",
    "                    break\n",
    "            if y_word[-1] in ['.', '?', '!']:\n",
    "                break\n",
    "        forward_sentences.append((' ').join(y_word))\n",
    "    \n",
    "    for m in keys:\n",
    "        length_bar = 5\n",
    "        ix_bar = [word_to_ix[m]]\n",
    "        y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "        relevant_words_bar = []\n",
    "        for j in range(len(articles)):\n",
    "            if y_word_bar[0] in articles.topics[j]:\n",
    "                for k in range(len(articles.content[j])):\n",
    "                    relevant_words_bar.append(articles.content[j][k])\n",
    "        x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "        for i in range(length_bar):\n",
    "            # appending the last predicted character to sequence\n",
    "            x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "            #print(ix_to_word[ix[-1]], end=\"\")\n",
    "            ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "            for l in range(len(ix_bar)):\n",
    "                if ix_to_word[ix_bar[-1-l]] in relevant_words_bar:\n",
    "                    y_word_bar.append(ix_to_word[ix_bar[-1-l]])\n",
    "                    break\n",
    "            if y_word_bar[-1] in ['.', '?', '!']:\n",
    "                del y_word_bar[-1]\n",
    "                break\n",
    "        y_word_bar = reversed(y_word_bar)\n",
    "        backward_sentences.append((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_paragraph = []\n",
    "for i in range(len(forward_sentences)):\n",
    "    generated_paragraph.append(backward_sentences[i] + ' '+ forward_sentences[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('Total processing time:', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
