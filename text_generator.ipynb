{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import fasttext as ft\n",
    "import re\n",
    "import itertools\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1 = pd.read_csv('articles1.csv')\n",
    "articles = articles1.iloc[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheetal/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#topic generator\n",
    "def topic_generation(x):\n",
    "    doc_complete = nltk.sent_tokenize(x)\n",
    "    stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "\n",
    "    doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "    dictionary = gensim.corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=len(doc_complete), id2word = dictionary, passes=20)\n",
    "    topic_list = []\n",
    "\n",
    "    topics = ldamodel.show_topics(num_topics = len(doc_complete), num_words = 1)\n",
    "    for i in range(len(topics)):\n",
    "        if topics[i][1][7:-1] not in topic_list:\n",
    "            topic_list.append(topics[i][1][7:-1])\n",
    "    return topic_list\n",
    "\n",
    "articles['topics'] = articles.content.map(lambda x:topic_generation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "def cleaner(x):\n",
    "    x = nltk.word_tokenize(str(x))\n",
    "    x = [w for w in x if not w in stopset]\n",
    "    x = [z.lower() for z in x]\n",
    "    return x\n",
    "\n",
    "articles.content = articles.content.map(lambda x:cleaner(x))\n",
    "articles.title = articles.title.map(lambda x:cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>[house, republicans, fret, about, winning, the...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[washington, —, congressional, republicans, ne...</td>\n",
       "      <td>[administration, ’s, republican, health, house...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>[rift, between, officers, residents, killings,...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[after, bullet, shells, get, counted, ,, blood...</td>\n",
       "      <td>[—, band, crime, call, 911, testify, squad, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>[tyrus, wong, ,, ‘, bambi, ’, artist, thwarted...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[when, walt, disney, ’, “, bambi, ”, opened, 1...</td>\n",
       "      <td>[art, child, 1942, character, —, animation, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>[among, deaths, 2016, ,, heavy, toll, pop, mus...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[death, may, great, equalizer, ,, ’, necessari...</td>\n",
       "      <td>[died, death, year, even, too, stage, actually...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>[kim, jong-un, says, north, korea, is, prepari...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[seoul, ,, south, korea, —, north, korea, ’, l...</td>\n",
       "      <td>[korea, north, mr, missile, ballistic, kim, hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  [house, republicans, fret, about, winning, the...   \n",
       "1           1  17284  [rift, between, officers, residents, killings,...   \n",
       "2           2  17285  [tyrus, wong, ,, ‘, bambi, ’, artist, thwarted...   \n",
       "3           3  17286  [among, deaths, 2016, ,, heavy, toll, pop, mus...   \n",
       "4           4  17287  [kim, jong-un, says, north, korea, is, prepari...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \\\n",
       "0  NaN  [washington, —, congressional, republicans, ne...   \n",
       "1  NaN  [after, bullet, shells, get, counted, ,, blood...   \n",
       "2  NaN  [when, walt, disney, ’, “, bambi, ”, opened, 1...   \n",
       "3  NaN  [death, may, great, equalizer, ,, ’, necessari...   \n",
       "4  NaN  [seoul, ,, south, korea, —, north, korea, ’, l...   \n",
       "\n",
       "                                              topics  \n",
       "0  [administration, ’s, republican, health, house...  \n",
       "1  [—, band, crime, call, 911, testify, squad, de...  \n",
       "2  [art, child, 1942, character, —, animation, on...  \n",
       "3  [died, death, year, even, too, stage, actually...  \n",
       "4  [korea, north, mr, missile, ballistic, kim, hu...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 95703 words\n",
      "Vocabulary size: 13829 words\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "word = []\n",
    "for i in range(len(articles.content)):\n",
    "    for j in range(len(articles.content[i])):\n",
    "        word.append(articles.content[i][j])\n",
    "        if articles.content[i][j] not in vocab:\n",
    "            vocab.append(articles.content[i][j])\n",
    "#chars = list(set(data))\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "seq_length = 1\n",
    "print('Data length: {} words'.format(len(word)))\n",
    "print('Vocabulary size: {} words'.format(VOCAB_SIZE))\n",
    "\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "    \n",
    "X = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "y = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "y_bar = np.zeros((int(len(word)/seq_length), seq_length, VOCAB_SIZE))\n",
    "for i in range(0, int(len(articles.content)/seq_length)):\n",
    "    X_sequence = word[i*seq_length:(i+1)*seq_length]\n",
    "    X_sequence_ix = [word_to_ix[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "        X[i] = input_sequence\n",
    "\n",
    "    y_sequence = word[i*seq_length+1:(i+1)*seq_length+1]\n",
    "    y_sequence_ix = [word_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1\n",
    "        y[i] = target_sequence\n",
    "        \n",
    "    y_bar_sequence = word[i*seq_length-1:(i+1)*seq_length-1]\n",
    "    y_bar_sequence_ix = [word_to_ix[value] for value in y_sequence]\n",
    "    target_sequence_bar = np.zeros((seq_length, VOCAB_SIZE))\n",
    "    for j in range(seq_length):\n",
    "        target_sequence_bar[j][y_sequence_ix[j]] = 1\n",
    "        y_bar[i] = target_sequence_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = X[:int(.75*len(X)), :, :]\n",
    "ytrain = y[:int(.75*len(X)), :, :]\n",
    "xtest = X[int(.75*len(X)):, :, :]\n",
    "ytest = y[int(.75*len(X)):, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[:int(.75*len(X)), :, :]\n",
    "y_train = y_bar[:int(.75*len(X)), :, :]\n",
    "x_test = X[int(.75*len(X)):, :, :]\n",
    "y_test = y_bar[int(.75*len(X)):, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(5000, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(6):\n",
    "  model.add(LSTM(int(4500/(i+1)), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71777 samples, validate on 23926 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, verbose=1, epochs=10, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.hdf5', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length = 5\n",
    "ix = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word = [ix_to_word[ix[-1]]]\n",
    "x = np.zeros((1, length, VOCAB_SIZE))\n",
    "for i in range(length):\n",
    "    # appending the last predicted character to sequence\n",
    "    x[0, i, :][ix[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "    y_word.append(ix_to_word[ix[-1]])\n",
    "    if y_word[-1] in ['.', '?', '!']:\n",
    "        break\n",
    "print((' ').join(y_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(5000, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(6):\n",
    "  model2.add(LSTM(int(4500/(i+1)), return_sequences=True))\n",
    "model2.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_BAR = 1000\n",
    "history_bar = model2.fit(x_train, y_train, batch_size=BATCH_SIZE_BAR, verbose=1, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights('model2.hdf5', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length_bar = 5\n",
    "ix_bar = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "for i in range(length_bar):\n",
    "    # appending the last predicted character to sequence\n",
    "    x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "    y_word_bar.append(ix_to_word[ix_bar[-1]])\n",
    "    if y_word_bar[-1] in ['.', '?', '!']:\n",
    "        del y_word_bar[-1]\n",
    "        break\n",
    "    \n",
    "y_word_bar = reversed(y_word_bar)\n",
    "print((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history_bar.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history_bar.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history_bar.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history_bar.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload model\n",
    "model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 1000\n",
    "#history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, verbose=1, epochs=10, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length = 5\n",
    "ix = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word = [ix_to_word[ix[-1]]]\n",
    "x = np.zeros((1, length, VOCAB_SIZE))\n",
    "for i in range(length):\n",
    "    # appending the last predicted character to sequence\n",
    "    x[0, i, :][ix[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "    y_word.append(ix_to_word[ix[-1]])\n",
    "    if y_word[-1] in ['.', '?', '!']:\n",
    "        break\n",
    "print((' ').join(y_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload model\n",
    "model2.load_weights('model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE_BAR = 1000\n",
    "#history = model2.fit(x_train, y_train, batch_size=BATCH_SIZE_BAR, verbose=1, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_sequence = pd.Series(words)\n",
    "length_bar = 5\n",
    "ix_bar = [np.random.randint(VOCAB_SIZE)]\n",
    "y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "for i in range(length_bar):\n",
    "    # appending the last predicted character to sequence\n",
    "    x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "    #print(ix_to_word[ix[-1]], end=\"\")\n",
    "    ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "    y_word_bar.append(ix_to_word[ix_bar[-1]])\n",
    "    if y_word_bar[-1] in ['.', '?', '!']:\n",
    "        del y_word_bar[-1]\n",
    "        break\n",
    "    \n",
    "y_word_bar = reversed(y_word_bar)\n",
    "print((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(str(\"Enter some keywords or sentences. In case you're entering keywords, don't use comma separation.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keywords.split()\n",
    "for i in keys:\n",
    "    if i not in word_to_ix:\n",
    "        keys.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if keys is None:\n",
    "    print('Error! No keyword recognized!')\n",
    "else:\n",
    "    forward_sentences = []\n",
    "    backward_sentences = []\n",
    "    for m in keys:\n",
    "        length = 5\n",
    "        ix = [word_to_ix[m]]\n",
    "        y_word = [ix_to_word[ix[-1]]]\n",
    "        relevant_words = []\n",
    "        for j in range(len(articles)):\n",
    "            if y_word in articles.topics[j]:\n",
    "                for k in range(len(articles.content[j])):\n",
    "                    relevant_words.append(articles.content[j][k])\n",
    "        x = np.zeros((1, length, VOCAB_SIZE))\n",
    "        for i in range(length):\n",
    "            # appending the last predicted character to sequence\n",
    "            x[0, i, :][ix[-1]] = 1\n",
    "            #print(ix_to_word[ix[-1]], end=\"\")\n",
    "            ix = np.argmax(model.predict(x[:, :i+1, :])[0], 1)\n",
    "            for l in range(len(ix)):\n",
    "                if ix_to_word[ix[-1-l]] in relevant_words:\n",
    "                    y_word.append(ix_to_word[ix[-1-l]])\n",
    "                    break\n",
    "            if y_word[-1] in ['.', '?', '!']:\n",
    "                break\n",
    "        forward_sentences.append((' ').join(y_word))\n",
    "    \n",
    "    for m in keys:\n",
    "        length_bar = 5\n",
    "        ix_bar = [word_to_ix[m]]\n",
    "        y_word_bar = [ix_to_word[ix_bar[-1]]]\n",
    "        relevant_words_bar = []\n",
    "        for j in range(len(articles)):\n",
    "            if y_word_bar in articles.topics[j]:\n",
    "                for k in range(len(articles.content[j])):\n",
    "                    relevant_words_bar.append(articles.content[j][k])\n",
    "        x_bar = np.zeros((1, length_bar, VOCAB_SIZE))\n",
    "        for i in range(length_bar):\n",
    "            # appending the last predicted character to sequence\n",
    "            x_bar[0, i, :][ix_bar[-1]] = 1\n",
    "            #print(ix_to_word[ix[-1]], end=\"\")\n",
    "            ix_bar = np.argmax(model2.predict(x_bar[:, :i+1, :])[0], 1)\n",
    "            for l in range(len(ix_bar)):\n",
    "                if ix_to_word[ix_bar[-1-l]] in relevant_words_bar:\n",
    "                    y_word_bar.append(ix_to_word[ix_bar[-1-l]])\n",
    "                    break\n",
    "            if y_word_bar[-1] in ['.', '?', '!']:\n",
    "                del y_word_bar[-1]\n",
    "                break\n",
    "        y_word_bar = reversed(y_word_bar)\n",
    "        backward_sentences.append((' ').join(y_word_bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_paragraph = []\n",
    "for i in range(len(forward_sentences)):\n",
    "    generated_paragraph.append(backward_sentences[i] + ' '+ forward_sentences[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
