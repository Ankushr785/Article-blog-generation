{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#import fasttext as ft\n",
    "import re\n",
    "import itertools\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import time\n",
    "import sys\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1 = pd.read_csv('articles_with_topics.csv')\n",
    "articles = articles1.iloc[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>['republican', 'administration', 'health', 'su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "      <td>['police', 'band', 'arrest', 'crime', 'percent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "      <td>['1942', 'wong', 'artistic', 'miserable', 'chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "      <td>['george', 'led', 'death', '“the', 'were', 'zs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>['ballistic', 'test', 'korea', 'missile', '1',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \\\n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...   \n",
       "1  NaN  After the bullet shells get counted, the blood...   \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...   \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...   \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...   \n",
       "\n",
       "                                              topics  \n",
       "0  ['republican', 'administration', 'health', 'su...  \n",
       "1  ['police', 'band', 'arrest', 'crime', 'percent...  \n",
       "2  ['1942', 'wong', 'artistic', 'miserable', 'chi...  \n",
       "3  ['george', 'led', 'death', '“the', 'were', 'zs...  \n",
       "4  ['ballistic', 'test', 'korea', 'missile', '1',...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(articles.content)\n",
    "def joinStrings(dictionary):\n",
    "    return ' '.join(string for string in dictionary)\n",
    "\n",
    "dictionary = joinStrings(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.lower()\n",
    "chars = sorted(list(set(dictionary)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 743978\n",
      "Total Vocab: 74\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(dictionary)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total characters:\", n_chars)\n",
    "print(\"Total Vocab:\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = dictionary[i:i+seq_length]\n",
    "    seq_out = dictionary[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "X_train = X[:int(0.75*len(X))]\n",
    "X_val = X[int(0.75*len(X)):]\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "y_train = y[:int(0.75*len(y))]\n",
    "y_val = y[int(0.75*len(y)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185970, 74)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "filepath=\"weights-improvement-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557908 samples, validate on 185970 samples\n",
      "Epoch 1/10\n",
      "  6144/557908 [..............................] - ETA: 50:04 - loss: 3.2262 - acc: 0.1460"
     ]
    }
   ],
   "source": [
    "start_training = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, callbacks=callbacks_list, validation_data = (X_val, y_val))\n",
    "finish_training = time.time()\n",
    "print('Training time = ', finish_training- start_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-10.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "def cleaner(x):\n",
    "    x = nltk.word_tokenize(str(x))\n",
    "    #x = [w for w in x if not w in stopset]\n",
    "    x = [z.lower() for z in x]\n",
    "    return x\n",
    "\n",
    "articles.content = articles.content.map(lambda x:cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "word = []\n",
    "for i in range(len(articles.content)):\n",
    "    for j in range(len(articles.content[i])):\n",
    "        word.append(articles.content[i][j])\n",
    "        if articles.content[i][j] not in vocab:\n",
    "            vocab.append(articles.content[i][j])\n",
    "#chars = list(set(data))\n",
    "seq_length = 1\n",
    "print('Data length: {} words'.format(len(word)))\n",
    "print('Vocabulary size: {} words'.format(len(vocab)))\n",
    "\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(str(\"Enter some keywords or sentences. In case you're entering keywords, don't use comma separation.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keywords.split()\n",
    "for i in keys:\n",
    "    if i not in word_to_ix:\n",
    "        keys.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if keys is None:\n",
    "    print('Error! No keyword recognized!')\n",
    "else:\n",
    "    article = []\n",
    "    for j in keys:\n",
    "        new_dictionary = []\n",
    "        for i in range(len(articles)):\n",
    "            if j in articles.topics[i]:\n",
    "                new_dictionary.append(articles.content[i])\n",
    "        def joinStrings(dictionary):\n",
    "            return ' '.join(string for string in dictionary)\n",
    "        new_dictionary = joinStrings(new_dictionary)\n",
    "        new_dictionary = new_dictionary.lower()\n",
    "        #chars_new = sorted(list(set(new_dictionary)))\n",
    "        #char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "        seq_length = 100\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for k in range(0, n_chars - seq_length, 1):\n",
    "            seq_in = dictionary[k:k+seq_length]\n",
    "            seq_out = dictionary[k+seq_length]\n",
    "            dataX.append([char_to_int[char] for char in seq_in])\n",
    "            dataY.append(char_to_int[seq_out])\n",
    "        n_patterns = len(dataX)\n",
    "        start = np.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX[start]\n",
    "        article.append(''.join([int_to_char[value] for value in pattern]))\n",
    "        # generate characters\n",
    "        for l in range(1000):\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            result = int_to_char[index]\n",
    "            article.append(result)\n",
    "            seq_in = [int_to_char[value] for value in pattern]\n",
    "            sys.stdout.write(result)\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generated article:', ''.join(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
